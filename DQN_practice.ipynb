{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor([[0 ,0, 0, 1], [1, 0, 1, 0], [0, 1, 0, 1], [ 1, 1, 1, 1], [ 1, 1, 1, 0]])\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0], [1]])\n",
    "# Y = torch.FloatTensor([0, 1, 1, 0, 1]).reshape(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = torch.nn.Linear(4, 2, bias=True)\n",
    "linear2 = torch.nn.Linear(2, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 :  0.000929579488002\n",
      "1000 :  0.000842447276227\n",
      "2000 :  0.000770144746639\n",
      "3000 :  0.000709243468009\n",
      "4000 :  0.000657259137370\n",
      "5000 :  0.000612328934949\n",
      "6000 :  0.000573127414100\n",
      "7000 :  0.000538651715033\n",
      "8000 :  0.000508090073708\n",
      "9000 :  0.000480786024127\n"
     ]
    }
   ],
   "source": [
    "for step in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(X)\n",
    "    \n",
    "    cost = criterion(outputs, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()    \n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print('%3d :  %15.15f' % (step, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y      yhat\n",
      "0  0.0  0.000644\n",
      "1  1.0  0.999486\n",
      "2  1.0  0.999493\n",
      "3  0.0  0.000616\n",
      "4  1.0  1.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pred = model(X)\n",
    "y = pd.Series(Y.flatten().numpy(), name='y')\n",
    "yhat = pd.Series(pred.detach().flatten().numpy(), name='yhat')\n",
    "comb = pd.concat([y, yhat], axis=1)\n",
    "print(comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### PangYoLap DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode :50, score : 10.2, n_buffer : 509, eps : 7.8%\n",
      "n_episode :100, score : 10.2, n_buffer : 1018, eps : 7.5%\n",
      "n_episode :150, score : 10.0, n_buffer : 1517, eps : 7.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-57-8e99e8b631ea>:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode :200, score : 10.0, n_buffer : 2018, eps : 7.0%\n",
      "n_episode :250, score : 14.9, n_buffer : 2762, eps : 6.8%\n",
      "n_episode :300, score : 23.0, n_buffer : 3913, eps : 6.5%\n",
      "n_episode :350, score : 21.7, n_buffer : 4999, eps : 6.2%\n",
      "n_episode :400, score : 12.9, n_buffer : 5642, eps : 6.0%\n",
      "n_episode :450, score : 12.5, n_buffer : 6265, eps : 5.8%\n",
      "n_episode :500, score : 12.1, n_buffer : 6870, eps : 5.5%\n",
      "n_episode :550, score : 19.6, n_buffer : 7851, eps : 5.3%\n",
      "n_episode :600, score : 42.3, n_buffer : 9967, eps : 5.0%\n",
      "n_episode :650, score : 35.3, n_buffer : 11731, eps : 4.8%\n",
      "n_episode :700, score : 91.1, n_buffer : 16288, eps : 4.5%\n",
      "n_episode :750, score : 152.9, n_buffer : 23934, eps : 4.2%\n",
      "n_episode :800, score : 179.5, n_buffer : 32911, eps : 4.0%\n",
      "n_episode :850, score : 190.4, n_buffer : 42432, eps : 3.8%\n",
      "n_episode :900, score : 197.3, n_buffer : 50000, eps : 3.5%\n",
      "n_episode :950, score : 193.2, n_buffer : 50000, eps : 3.2%\n",
      "n_episode :1000, score : 207.9, n_buffer : 50000, eps : 3.0%\n",
      "n_episode :1050, score : 162.0, n_buffer : 50000, eps : 2.8%\n",
      "n_episode :1100, score : 130.2, n_buffer : 50000, eps : 2.5%\n",
      "n_episode :1150, score : 165.8, n_buffer : 50000, eps : 2.2%\n",
      "n_episode :1200, score : 171.1, n_buffer : 50000, eps : 2.0%\n",
      "n_episode :1250, score : 162.3, n_buffer : 50000, eps : 1.8%\n",
      "n_episode :1300, score : 189.0, n_buffer : 50000, eps : 1.5%\n",
      "n_episode :1350, score : 163.4, n_buffer : 50000, eps : 1.2%\n",
      "n_episode :1400, score : 160.6, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1450, score : 143.1, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1500, score : 168.7, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1550, score : 228.4, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1600, score : 262.3, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1650, score : 218.4, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1700, score : 236.4, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1750, score : 168.7, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1800, score : 171.0, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1850, score : 252.0, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1900, score : 78.0, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :1950, score : 208.9, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :2000, score : 221.6, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :2050, score : 249.0, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :2100, score : 217.1, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :2150, score : 257.4, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :2200, score : 213.0, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :2250, score : 229.2, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :2300, score : 136.4, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :2350, score : 265.8, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :2400, score : 220.0, n_buffer : 50000, eps : 1.0%\n",
      "n_episode :2450, score : 189.9, n_buffer : 50000, eps : 1.0%\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma         = 0.98\n",
    "buffer_limit  = 50000\n",
    "batch_size    = 32\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "    \n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, n): # 버퍼에서 샘플링\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "               torch.tensor(done_mask_lst)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)   \n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x): # Q Value 리턴 (음수가 될 수 도 있음)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))    \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "      \n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs)\n",
    "        coin = random.random() # 0 ~ 1 \n",
    "        if coin < epsilon:\n",
    "            return random.randint(0,1)\n",
    "        else : \n",
    "            return out.argmax().item()\n",
    "            \n",
    "def train(q, q_target, memory, optimizer):\n",
    "    for i in range(10):\n",
    "        s, a, r, s_prime, done_mask = memory.sample(batch_size)\n",
    "\n",
    "        q_out = q(s) # input size (32,4) return size (32,2)\n",
    "        q_a = q_out.gather(1, a) # 취한 액션의 큐값만 골라냄 (32,1)\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r + gamma * max_q_prime * done_mask\n",
    "        loss = F.smooth_l1_loss(q_a, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "q = Qnet()\n",
    "q_target = Qnet()\n",
    "q_target.load_state_dict(q.state_dict())\n",
    "memory = ReplayBuffer()\n",
    "\n",
    "print_interval = 50\n",
    "score = 0.0  \n",
    "optimizer = optim.Adam(q.parameters(), lr=learning_rate) # q_target 은 업데이트 안 함!\n",
    "\n",
    "for n_epi in range(2500):\n",
    "    epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) #Linear annealing from 8% to 1%\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        a = q.sample_action(torch.from_numpy(s).float(), epsilon)      \n",
    "        s_prime, r, done, info = env.step(a)\n",
    "        done_mask = 0.0 if done else 1.0\n",
    "        memory.put((s,a,r/100.0,s_prime, done_mask))\n",
    "        s = s_prime\n",
    "\n",
    "        score += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if memory.size()>2000:\n",
    "        train(q, q_target, memory, optimizer)\n",
    "\n",
    "    if n_epi%print_interval==0 and n_epi!=0:\n",
    "        q_target.load_state_dict(q.state_dict()) # 타겟 네트워크 업데이트 (20 번 에피소드 마다)\n",
    "        print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(n_epi, score/print_interval, memory.size(), epsilon*100))                \n",
    "        \n",
    "        if (score/print_interval) > 300:\n",
    "            break\n",
    "            \n",
    "        score = 0.0\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\shinki\\\\OneDrive - Novelis Inc\\\\Documents\\\\Data Science\\\\10. Learning\\\\7. RL\\\\q_target'\n",
    "# # torch.save(q_target.state_dict(), path) # save weights only\n",
    "# torch.save(q_target, path)\n",
    "q_target = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 367 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i_episode in range(5):\n",
    "    observation = env.reset()\n",
    "    for t in range(550):\n",
    "        time.sleep(0.01)\n",
    "        env.render()\n",
    "        action = q_target(torch.Tensor(observation)).argmax().item() \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            time.sleep(1)\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Random or Simple Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 36 timesteps\n",
      "Episode finished after 53 timesteps\n",
      "Episode finished after 28 timesteps\n",
      "Episode finished after 39 timesteps\n",
      "Episode finished after 34 timesteps\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "for i_episode in range(5):\n",
    "    observation = env.reset()\n",
    "    for t in range(300):\n",
    "        time.sleep(0.01)\n",
    "        env.render()\n",
    "#         action = env.action_space.sample()\n",
    "#         action = random.randint(0,1)\n",
    "\n",
    "        if observation[0] > 0: # if the pole is on the right side\n",
    "            action = 0 #  pushing the cart to the left\n",
    "        else:\n",
    "            action = 1 #  pushing the cart to the right\n",
    "        else:\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            time.sleep(1)\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Physics !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n"
     ]
    }
   ],
   "source": [
    "def theta_omega_policy(obs):\n",
    "    theta, w = obs[2:4]\n",
    "    if abs(theta) < 0.03:\n",
    "        return 0 if w < 0 else 1\n",
    "    else:\n",
    "        return 0 if theta < 0 else 1\n",
    "    \n",
    "import time\n",
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "for i_episode in range(5):\n",
    "    observation = env.reset()\n",
    "    for t in range(550):\n",
    "        time.sleep(0.01)\n",
    "        env.render()\n",
    "        action = theta_omega_policy(observation)\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            time.sleep(1)\n",
    "            break\n",
    "env.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.35309133  0.5595656  -0.0241827  -0.9231632 ] 1.0\n"
     ]
    }
   ],
   "source": [
    "observation, reward, done, info = env.step(1)\n",
    "print(observation, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03732754 -0.1904975   0.0271492   0.26085016] 1.0\n",
      "[-0.04113749 -0.38599628  0.03236621  0.5619711 ] 1.0\n",
      "[-0.04885742 -0.58155715  0.04360563  0.8646728 ] 1.0\n",
      "[-0.06048856 -0.7772446   0.06089909  1.1707411 ] 1.0\n",
      "[-0.07603345 -0.97310334  0.08431391  1.4818778 ] 1.0\n",
      "[-0.09549552 -1.1691467   0.11395146  1.799657  ] 1.0\n",
      "[-0.11887845 -1.3653438   0.1499446   2.1254735 ] 1.0\n",
      "[-0.14618532 -1.5616052   0.19245407  2.460481  ] 1.0\n",
      "[-0.17741743 -1.7577648   0.2416637   2.805521  ] 1.0\n",
      "[-0.21257272 -1.953561    0.2977741   3.1610403 ] 0.0\n",
      "[-0.25164396 -2.1486168   0.3609949   3.5270054 ] 0.0\n",
      "[-0.29461628 -2.342422    0.43153504  3.9028182 ] 0.0\n",
      "[-0.34146473 -2.5343204   0.5095914   4.2872477 ] 0.0\n",
      "[-0.39215112 -2.7235076   0.5953363   4.6783915 ] 0.0\n",
      "[-0.4466213  -2.9090443   0.68890417  5.0736885 ] 0.0\n",
      "[-0.50480217 -3.0898914   0.7903779   5.4699874 ] 0.0\n",
      "[-0.5666    -3.264971   0.8997777  5.8636823] 0.0\n",
      "[-0.6318994 -3.4332526  1.0170513  6.250892 ] 0.0\n",
      "[-0.70056444 -3.5938635   1.1420692   6.627649  ] 0.0\n",
      "[-0.77244174 -3.746211    1.2746222   6.9900403 ] 0.0\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "for i in range(20):\n",
    "\n",
    "#     action = random.randint(0,1)\n",
    "    if observation[0] > 0:\n",
    "        action = 1\n",
    "    else:\n",
    "        action = 0\n",
    "        \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(observation, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
